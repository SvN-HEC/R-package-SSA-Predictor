mssa_indicator_obj<-compute_mssa_BIP_predictors_func(x_mat,lambda_HP,L,date_to_fit,p,q,ht_mssa_vec,h_vec,f_excess,lag_vec,select_vec_multi)
# Retrieve predictors and targets from the above function-call
# Forward-shifted HP-BIP
target_shifted_mat=mssa_indicator_obj$target_shifted_mat
# M-SSA predictors: for each forecast horizon h( the columns of predictor_mssa_mat), the M-SSA predictor is
#   obtained as equally-weighted average of all series outputs: BIP, ip,..., see exercise 3 of tutorial 7.2
predictor_mssa_mat<-mssa_indicator_obj$predictor_mssa_mat
# M-MSE predictors
predictor_mmse_mat<-mssa_indicator_obj$predictor_mmse_mat
# Sub-series of M-SSA predictors: for each forecast horizon h, the M-SSA predictor is obtained as equally-weighted
#   average of all series outputs: BIP, ip,...
# The components or sub-series of the aggregate predictor are contained in mssa_array
# This is a 3-dim array with dimensions: sub-series (BIP, ip,...), time, forecast horizon h in h_vec
#   -We shall see below that the sub-series can be important when interpreting the M-SSA predictors
mssa_array<-mssa_indicator_obj$mssa_array
# Plot M-SSA: the vertical line indicates the end of the in-sample span
mplot<-predictor_mssa_mat
colnames(mplot)<-colnames(predictor_mssa_mat)
par(mfrow=c(1,1))
colo<-c(rainbow(ncol(predictor_mssa_mat)))
main_title<-c(paste("Standardized M-SSA predictors for forecast horizons ",paste(h_vec,collapse=","),sep=""),"Vertical line delimites in-sample and out-of-sample span")
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (j in 1:ncol(mplot))
{
lines(mplot[,j],col=colo[j],lwd=1,lty=1)
mtext(colnames(mplot)[j],col=colo[j],line=-j)
}
abline(h=0)
abline(v=which(rownames(mplot)>date_to_fit)[1]-1,lty=2)
axis(1,at=c(1,12*1:(nrow(mplot)/12)),labels=rownames(mplot)[c(1,12*1:(nrow(mplot)/12))])
axis(2)
box()
# 1.2. Compute performances
# 1.2.1 Specify the benchmarks against which M-SSA will be compared
#   -We compare M-SSA against the mean (of BIP) and against a classic direct forecast
#   -The direct forecast is based on regressing a selection of macro indicators on forward-shifted BIP
# We can specify the selection of macro-indicators
select_direct_indicator<-c("ifo_c","ESI")
# Note: too complex designs (too many indicators) lead to overfitting and thus worse out-of-sample performances
# To illustrate the direct predictor consider the following example of a h-step ahead direct forecast:
h<-2
# Shift BIP forward by publication lag+forecast horizon
forward_shifted_BIP<-c(x_mat[(1+lag_vec[1]+h):nrow(x_mat),"BIP"],rep(NA,h+lag_vec[1]))
# Regress selected indicators on forward-shifted BIP
lm_obj<-lm(forward_shifted_BIP~x_mat[,select_direct_indicator])
# You will probably not find statistically significant regressors for h>2: BIP is a very noisy series
summary(lm_obj)
# Technical note:
# -Residuals are subject to heteroscedasticity (crises) and autocorrelation
# -Therefore classic OLS tests for statistical significance are biased
# -We shall rely on HAC-adjusted p-values further down (R-package sandwich)
# Compute the predictor: one can rely on the generic R-function predict or compute the predictor manually
direct_forecast<-lm_obj$coef[1]+x_mat[,select_direct_indicator]%*%lm_obj$coef[2:(length(select_direct_indicator)+1)]
# Note that this is a full-sample predictor (no out-of-sample span)
# We can now plot target and direct forecast: for h>2 the predictor comes close to a flat line centered at zero
ts.plot(cbind(forward_shifted_BIP,direct_forecast),main=paste("BIP shifted forward by ",h," quarters (black) vs. direct forecast (red)"),col=c("black","red"))
abline(h=0)
# 1.2.2 Compute performances
# The following function computes direct forecasts and evaluates M-SSA against mean(BIP) and direct forecasts
# -Performance measures:
#   -Target correlations (correlations of predictors with targets)
#     -Correlations emphasize the dynamic aspects of forecasts: they ignore static level and scale adjustments
#   -Relative root mean-square errors: potential gains of M-SSA over mean(BIP) and direct forecasts
#   -HAC-adjusted p-values:
#     -we regress the predictor on forward-shifted BIP and compute the t-statistic
#       of the regression coefficient
#     -Small p-values indicate statistical significance of the predictor
#     -Since target and predictor are subject to heteroscedasticity (COVID-outliers...) and autocorrelation
#       we here rely on HAC-adjusted p-values (R-package sandwich)
#   -We compute full-sample and out-of-sample results: the out-of-sample span is data after date_to_fit specified above
#   -We consider two different targets
#     a. Foreward-shifted HP applied to BIP: HP_BIP. This is the target for which M-SSA has been optimized
#     b. Forward-shifted BIP, i.e., we include the (unpredictable) noisy high-frequency part of BIP
# Note:
#   -M-SSA does not target BIP directly (overfitting is not of concern)
perf_obj<-compute_perf_func(x_mat,target_shifted_mat,predictor_mssa_mat,predictor_mmse_mat,date_to_fit,select_direct_indicator,h_vec)
# Retrieve all performance measures
p_value_HAC_HP_BIP_full=perf_obj$p_value_HAC_HP_BIP_full
t_HAC_HP_BIP_full=perf_obj$t_HAC_HP_BIP_full
cor_mat_HP_BIP_full=perf_obj$cor_mat_HP_BIP_full
p_value_HAC_HP_BIP_oos=perf_obj$p_value_HAC_HP_BIP_oos
t_HAC_HP_BIP_oos=perf_obj$t_HAC_HP_BIP_oos
cor_mat_HP_BIP_oos=perf_obj$cor_mat_HP_BIP_oos
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
t_HAC_BIP_full=perf_obj$t_HAC_BIP_full
cor_mat_BIP_full=perf_obj$cor_mat_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
t_HAC_BIP_oos=perf_obj$t_HAC_BIP_oos
cor_mat_BIP_oos=perf_obj$cor_mat_BIP_oos
rRMSE_MSSA_HP_BIP_direct=perf_obj$rRMSE_MSSA_HP_BIP_direct
rRMSE_MSSA_HP_BIP_mean=perf_obj$rRMSE_MSSA_HP_BIP_mean
rRMSE_MSSA_BIP_direct=perf_obj$rRMSE_MSSA_BIP_direct
rRMSE_MSSA_BIP_mean=perf_obj$rRMSE_MSSA_BIP_mean
target_BIP_mat=perf_obj$target_BIP_mat
# Retrieve all performance measures
p_value_HAC_HP_BIP_full=perf_obj$p_value_HAC_HP_BIP_full
t_HAC_HP_BIP_full=perf_obj$t_HAC_HP_BIP_full
cor_mat_HP_BIP_full=perf_obj$cor_mat_HP_BIP_full
p_value_HAC_HP_BIP_oos=perf_obj$p_value_HAC_HP_BIP_oos
t_HAC_HP_BIP_oos=perf_obj$t_HAC_HP_BIP_oos
cor_mat_HP_BIP_oos=perf_obj$cor_mat_HP_BIP_oos
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
t_HAC_BIP_full=perf_obj$t_HAC_BIP_full
cor_mat_BIP_full=perf_obj$cor_mat_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
t_HAC_BIP_oos=perf_obj$t_HAC_BIP_oos
cor_mat_BIP_oos=perf_obj$cor_mat_BIP_oos
rRMSE_MSSA_HP_BIP_direct=perf_obj$rRMSE_MSSA_HP_BIP_direct
rRMSE_MSSA_HP_BIP_mean=perf_obj$rRMSE_MSSA_HP_BIP_mean
rRMSE_MSSA_BIP_direct=perf_obj$rRMSE_MSSA_BIP_direct
rRMSE_MSSA_BIP_mean=perf_obj$rRMSE_MSSA_BIP_mean
target_BIP_mat=perf_obj$target_BIP_mat
#------------
# 1.2.3 Evaluation
# We first look at the target correlations between M-SSA predictors and forward-shifted HP-BIP (including the publication lag)
# a. Full sample
cor_mat_HP_BIP_full
# We can recognize a systematic pattern:
#   -For increasing forward-shift (from top to bottom in the above matrix) M-SSA designs optimized for
# b. Out-of-sample (period following estimation span for VAR-model of M-SSA)
cor_mat_HP_BIP_oos
# Similar to full-sample
# We now look at the target correlation when the target is forward-shifted BIP (instead of HP-BIP)
cor_mat_BIP_full
cor_mat_BIP_oos
p_value_HAC_BIP_oos
# Remarks:
#   1. Since M-SSA predictors are standardized (equal-weighting cross-sectional aggregation), we need to
#         calibrate them by regression onto the target (to determine static level and scale parameters)
#   2. As a result, rRMSE of M-SSA against mean(BIP) and the above target correlations are redundant statistics:
#         the information content is the same but it is presented in an alternative, slightly different form
#   3. Background:
#       -The M-SSA objective function is the target correlation (not the mean-square error)
#       -Therefore, M-SSA ignores static level and scale adjustments
#   4. Root mean-square errors are evaluated on the out-of-sample span only (specified by date_to_fit)
#   5. The benchmark direct predictors are full-sample estimates
#       -Estimates based on short in-sample spans are unreliable (insignificant regression coefficients)
#   6. The benchmark mean-predictor used in our comparisons is based on the mean of the target in the
#       out-of-sample span (it is looking ahead)
#   7. The main purpose of these comparisons is to evaluate the dynamic capability of the M-SSA predictor out-of-sample
#       -The (ex post) static level and scale adjustments are deemed less relevant
# With these remarks in mind let's begin:
# The first rRMSE emphasizes M-SSA vs. the mean benchmark (of BIP), both targeting HP-BIP:
#   -Numbers smaller one signify an outperformance of M-SSA against the mean-benchmark when targeting HP-BIP
# All metrics are out-of-sample
rRMSE_MSSA_HP_BIP_mean
# We next look at M-SSA vs. direct predictors based on indicators selected  in select_direct_indicator: targeting HP-BIP
rRMSE_MSSA_HP_BIP_direct
# Next: M-SSA vs. mean (of BIP) when targeting BIP
rRMSE_MSSA_BIP_mean
rRMSE_MSSA_BIP_direct
# The above correlations and rRMSE do not test for statistical significance (of predictability)
# The following HAC-adjusted p-values provide a way to infer statistical significance
# We look at HAC-adjusted p-values of regressions of M-SSA on forward-shifted targets
# Remarks:
#   -In some cases the HAC standard error (of the regression coefficient) seems `suspicious'
#     -HAC estimate of standard error could be substantially smaller than the ordinary OLS/unadjusted estimate
#   -We therefore compute both types of standard errors and we rely on the maximum for a derivation of p-values
#   -In this sense our p-values may be claimed to be `conservative'
# We first consider forward-shifted HP-BIP and full sample p-values
p_value_HAC_HP_BIP_full
# Out-of-sample:
p_value_HAC_HP_BIP_oos
# Same as above but now targeting BIP
# Full sample
p_value_HAC_BIP_full
p_value_HAC_BIP_oos
# 1.3 Visualize performances: link performance measures to plots of predictors against targets
# 1.3.1 M-SSA predictors (without sub-series)
# Select a forward-shift of target (the k-th entry in h_vec)
k<-5
# This is the forecast horizon
h_vec[k]
# To obtain the effective forward-shift we have to add the publication lag
shift<-h_vec[k]+lag_vec[1]
# Effective forward shift of BIP
shift
if (k>length(h_vec))
{
print(paste("k should be smaller equal ",length(h_vec),sep=""))
k<-length(h_vec)
}
# Select a M-SSA predictor: optimized for forecast horizon h_vec[j]
j<-k
if (j>length(h_vec))
{
print(paste("j should be smaller equal ",length(h_vec),sep=""))
j<-length(h_vec)
}
# Plot targets (forward-shifted BIP and HP-BIP) and predictor
par(mfrow=c(1,1))
# Scale the data for better visual interpretation of effect of excess forecast on M-SSA (red) vs. previous M-SSA (blue)
mplot<-scale(cbind(target_BIP_mat[,k],target_shifted_mat[,k],predictor_mssa_mat[,j]))
rownames(mplot)<-rownames(x_mat)
colnames(mplot)<-c(paste("BIP left-shifted by ",h_vec[k]," quarters (plus publication lag)",sep=""),paste("HP-BIP left-shifted by ",h_vec[k]," quarters (plus publication lag)",sep=""),paste("M-SSA predictor optimized for h=",h_vec[j],sep=""))
colo<-c("black","violet","blue")
main_title<-"Standardized forward-shifted BIP and HP-BIP vs. M-SSA predictor"
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=c(2,rep(1,ncol(x_mat)-1)),ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i],lwd=1,lty=1)
mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
abline(v=which(rownames(mplot)<=date_to_fit)[length(which(rownames(mplot)<=date_to_fit))],lwd=2,lty=2)
axis(1,at=c(1,4*1:(nrow(mplot)/4)),labels=rownames(mplot)[c(1,4*1:(nrow(mplot)/4))])
axis(2)
box()
cor(na.exclude(mplot))[2,ncol(mplot)]
# This number corresponds to element (k,j) of the matrix cor_mat_HP_BIP_full computed by our function
cor_mat_HP_BIP_full[k,j]
# We can also look at the correlation of the predictor with the forward-shifted BIP (instead of HP-BIP)
cor(na.exclude(mplot))[1,3]
# Note: the (k,j) entry of cor_mat_BIP generally differs from cor(na.exclude(mplot))[1,3]
cor_mat_BIP_full[k,j]
# The reason is simple: by removing NAs (due to inclusion of the two-sided target in mplot) we change the sample-size
# We can easily correct by removing the two-sided target in mplot
mplot_without_two_sided<-scale(cbind(target_BIP_mat[,k],predictor_mssa_mat[,j]))
# This number now matches cor_mat_BIP[k,j]
cor(na.exclude(mplot_without_two_sided))[1,2]
p_value_HAC_BIP_full[k,j]
# 2. Out-of-sample
p_value_HAC_BIP_oos[k,j]
# Not significant at a one-year ahead horizon when targeting BIP (which is a very noisy series...)
# Instead of BIP we might have a look at targeting HP-BIP (also shifted one year ahead)
p_value_HAC_HP_BIP_full[k,j]
p_value_HAC_HP_BIP_oos[k,j]
# -For a given forecast horizon h in h_vec, we can look at the sub-series entering (equally-weighted into) the M-SSA predictor
#   -The sub-series are potentially informative when interpreting the aggregate predictor
#   -We can examine which sub-series is more/less likely to trigger a dynamic change of the predictor/nowcast
# -For discussion, we now select the M-SSA nowcast
j_now<-1
# This is the forecast horizon (nowcast)
h_vec[j_now]
# -For forecast horizon h_vec[j_now], the sub-series of the M-SSA predictor are:
tail(t(mssa_array[,,j_now]))
# These sub-series correspond to the outputs of the multivariate M-SSA optimized for horizon h_vec[j_now]
#   -One output for each series of the multivariate design
# We can check that the M-SSA predictor is the cross-sectional mean of the standardized sub-series:
#   -The following maximal error/deviation should be `small` (zero up to numerical precision)
max(abs(apply(scale(t(mssa_array[,,j_now])),1,mean)-predictor_mssa_mat[,j_now]),na.rm=T)
# Plot M-SSA nowcast and sub-series
par(mfrow=c(1,1))
# Scale the data
mplot<-scale(cbind(predictor_mssa_mat[,j_now],scale(t(mssa_array[,,j_now]))))
rownames(mplot)<-rownames(x_mat)
colnames(mplot)<-c(paste("M-SSA predictor optimized for h=",h_vec[j_now],sep=""),
paste("Subseries ",select_vec_multi,sep=""))
colo<-c("blue",rainbow(length(select_vec_multi)))
main_title<-c(paste("M-SSA predictor for h=",h_vec[j_now]," (solid blue) and sub-series (dashed lines)",sep=""),"In-sample span up to black vertical (dashed) line")
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=2,ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i],lwd=1,lty=2)
mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
abline(v=which(rownames(mplot)<=date_to_fit)[length(which(rownames(mplot)<=date_to_fit))],lwd=2,lty=2)
axis(1,at=c(1,4*1:(nrow(mplot)/4)),labels=rownames(mplot)[c(1,4*1:(nrow(mplot)/4))])
axis(2)
box()
# 2.1
# Generate artificial white noise data
# One can try multiple set.seed
# This one will generate multiple significant results (p<5%) for the out-of-sample span, but none below 1%
set.seed(1)
# This one will generate only one p-value below 5% in the full sample span and none below 1%
set.seed(2)
# None below 1%
set.seed(3)
# None below 1%
set.seed(4)
# None below 1%
set.seed(5)
# None below 1%
set.seed(9)
# None below 1%
set.seed(6)
# One single p-value below 1%
set.seed(7)
# None below 1%
set.seed(8)
# None below 1%
set.seed(9)
# None below 1%
set.seed(10)
# The outcome suggests that HAC-adjustments are unable to correct fully for data-dependence
#   -We observe p-values below 5% more often than in 5% of all cases
#   -Therefore, some care is needed when evaluating results on the verge of statistical significance
# However, our results also suggest that p-values below 1% are `rare`
#   -We found only one p-value below 1% for the above set.seeds, out of 10*5*5*2=500 computed values
x_mat_white_noise<-NULL
for (i in 1:ncol(x_mat))
x_mat_white_noise<-cbind(x_mat_white_noise,rnorm(nrow(x_mat)))
# Provide colnames and rownames from x_mat: necessary because the function relies on dates and column names of selected indicators
rownames(x_mat_white_noise)<-rownames(x_mat)
colnames(x_mat_white_noise)<-colnames(x_mat)
tail(x_mat_white_noise)
# Check ACF
acf(x_mat_white_noise)
# 2.2 Apply M-SSA
# Target filter: lambda_HP is the single most important hyperparameter, see tutorial 7.1 for a discussion
# Briefly: we avoid the classic quarterly setting lambda_HP=1600 because the resulting filter would be too smooth
# Too smooth means: the forecast horizon would have nearly no effect on the M-SSA predictor (almost no left-shift, no anticipation)
lambda_HP<-160
# Filter length: nearly 8 years is fine for the selected lambda_HP (filter weights decay sufficiently fast)
L<-31
# In-sample span for VAR, i.e., M-SSA (the proposed design is quite insensitive to this specification because the VAR is parsimoniously parameterized)
date_to_fit<-"2008"
# VARMA model orders: keep the model simple in particular for short/tight in-sample spans
p<-0
q<-0
# Holding-times (HT): controls smoothness of M-SSA (the following numbers are pasted from the original predictor)
# Increasing these numbers leads to predictors with less zero-crossings (smoother)
ht_mssa_vec<-c(6.380160,  6.738270,   7.232453,   7.225927,   7.033768)
names(ht_mssa_vec)<-colnames(x_mat)
# Forecast horizons: M-SSA is optimized for each forecast horizon in h_vec
h_vec<-0:6
# Forecast excesses: see tutorial 7.1 for background
f_excess<-rep(4,length(select_vec_multi))
# Run the function packing and implementing our previous findings (tutorial 7.2)
mssa_indicator_obj<-compute_mssa_BIP_predictors_func(x_mat_white_noise,lambda_HP,L,date_to_fit,p,q,ht_mssa_vec,h_vec,f_excess,lag_vec,select_vec_multi)
# 3.3 Check performances:
# Forward-shifted HP-BIP
target_shifted_mat=mssa_indicator_obj$target_shifted_mat
# M-SSA indicators
predictor_mssa_mat<-mssa_indicator_obj$predictor_mssa_mat
# M-MSE
predictor_mmse_mat<-mssa_indicator_obj$predictor_mmse_mat
# For the direct predictor we can specify the macro-indicators in the expanding-window regressions
#   -Note: too complex designs lead to overfitting and thus worse out-of-sample performances
select_direct_indicator<-c("ifo_c","ESI")
perf_obj<-compute_perf_func(x_mat_white_noise,target_shifted_mat,predictor_mssa_mat,predictor_mmse_mat,date_to_fit,select_direct_indicator,h_vec)
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
# We need to check whether we can forecast the white noise data WN (not HP-WN)  based on the predictors
# Full sample
p_value_HAC_BIP_full
# Out-of-sample:
p_value_HAC_BIP_oos
# Check number of occurrences below 1%
length(which(p_value_HAC_BIP_full<0.01))
length(which(p_value_HAC_BIP_oos<0.01))
set.seed(9)
# The outcome suggests that HAC-adjustments are unable to correct fully for data-dependence
#   -We observe p-values below 5% more often than in 5% of all cases
#   -Therefore, some care is needed when evaluating results on the verge of statistical significance
# However, our results also suggest that p-values below 1% are `rare`
#   -We found only one p-value below 1% for the above set.seeds, out of 10*5*5*2=500 computed values
x_mat_white_noise<-NULL
for (i in 1:ncol(x_mat))
x_mat_white_noise<-cbind(x_mat_white_noise,rnorm(nrow(x_mat)))
# Provide colnames and rownames from x_mat: necessary because the function relies on dates and column names of selected indicators
rownames(x_mat_white_noise)<-rownames(x_mat)
colnames(x_mat_white_noise)<-colnames(x_mat)
tail(x_mat_white_noise)
# Check ACF
acf(x_mat_white_noise)
# 2.2 Apply M-SSA
# Target filter: lambda_HP is the single most important hyperparameter, see tutorial 7.1 for a discussion
# Briefly: we avoid the classic quarterly setting lambda_HP=1600 because the resulting filter would be too smooth
# Too smooth means: the forecast horizon would have nearly no effect on the M-SSA predictor (almost no left-shift, no anticipation)
lambda_HP<-160
# Filter length: nearly 8 years is fine for the selected lambda_HP (filter weights decay sufficiently fast)
L<-31
# In-sample span for VAR, i.e., M-SSA (the proposed design is quite insensitive to this specification because the VAR is parsimoniously parameterized)
date_to_fit<-"2008"
# VARMA model orders: keep the model simple in particular for short/tight in-sample spans
p<-0
q<-0
# Holding-times (HT): controls smoothness of M-SSA (the following numbers are pasted from the original predictor)
# Increasing these numbers leads to predictors with less zero-crossings (smoother)
ht_mssa_vec<-c(6.380160,  6.738270,   7.232453,   7.225927,   7.033768)
names(ht_mssa_vec)<-colnames(x_mat)
# Forecast horizons: M-SSA is optimized for each forecast horizon in h_vec
h_vec<-0:6
# Forecast excesses: see tutorial 7.1 for background
f_excess<-rep(4,length(select_vec_multi))
# Run the function packing and implementing our previous findings (tutorial 7.2)
mssa_indicator_obj<-compute_mssa_BIP_predictors_func(x_mat_white_noise,lambda_HP,L,date_to_fit,p,q,ht_mssa_vec,h_vec,f_excess,lag_vec,select_vec_multi)
# 3.3 Check performances:
# Forward-shifted HP-BIP
target_shifted_mat=mssa_indicator_obj$target_shifted_mat
# M-SSA indicators
predictor_mssa_mat<-mssa_indicator_obj$predictor_mssa_mat
# M-MSE
predictor_mmse_mat<-mssa_indicator_obj$predictor_mmse_mat
# For the direct predictor we can specify the macro-indicators in the expanding-window regressions
#   -Note: too complex designs lead to overfitting and thus worse out-of-sample performances
select_direct_indicator<-c("ifo_c","ESI")
perf_obj<-compute_perf_func(x_mat_white_noise,target_shifted_mat,predictor_mssa_mat,predictor_mmse_mat,date_to_fit,select_direct_indicator,h_vec)
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
# We need to check whether we can forecast the white noise data WN (not HP-WN)  based on the predictors
# Full sample
p_value_HAC_BIP_full
# Out-of-sample:
p_value_HAC_BIP_oos
# Check number of occurrences below 1%
length(which(p_value_HAC_BIP_full<0.01))
length(which(p_value_HAC_BIP_oos<0.01))
set.seed(8)
# The outcome suggests that HAC-adjustments are unable to correct fully for data-dependence
#   -We observe p-values below 5% more often than in 5% of all cases
#   -Therefore, some care is needed when evaluating results on the verge of statistical significance
# However, our results also suggest that p-values below 1% are `rare`
#   -We found only one p-value below 1% for the above set.seeds, out of 10*5*5*2=500 computed values
x_mat_white_noise<-NULL
for (i in 1:ncol(x_mat))
x_mat_white_noise<-cbind(x_mat_white_noise,rnorm(nrow(x_mat)))
# Provide colnames and rownames from x_mat: necessary because the function relies on dates and column names of selected indicators
rownames(x_mat_white_noise)<-rownames(x_mat)
colnames(x_mat_white_noise)<-colnames(x_mat)
tail(x_mat_white_noise)
# Check ACF
acf(x_mat_white_noise)
# 2.2 Apply M-SSA
# Target filter: lambda_HP is the single most important hyperparameter, see tutorial 7.1 for a discussion
# Briefly: we avoid the classic quarterly setting lambda_HP=1600 because the resulting filter would be too smooth
# Too smooth means: the forecast horizon would have nearly no effect on the M-SSA predictor (almost no left-shift, no anticipation)
lambda_HP<-160
# Filter length: nearly 8 years is fine for the selected lambda_HP (filter weights decay sufficiently fast)
L<-31
# In-sample span for VAR, i.e., M-SSA (the proposed design is quite insensitive to this specification because the VAR is parsimoniously parameterized)
date_to_fit<-"2008"
# VARMA model orders: keep the model simple in particular for short/tight in-sample spans
p<-0
q<-0
# Holding-times (HT): controls smoothness of M-SSA (the following numbers are pasted from the original predictor)
# Increasing these numbers leads to predictors with less zero-crossings (smoother)
ht_mssa_vec<-c(6.380160,  6.738270,   7.232453,   7.225927,   7.033768)
names(ht_mssa_vec)<-colnames(x_mat)
# Forecast horizons: M-SSA is optimized for each forecast horizon in h_vec
h_vec<-0:6
# Forecast excesses: see tutorial 7.1 for background
f_excess<-rep(4,length(select_vec_multi))
# Run the function packing and implementing our previous findings (tutorial 7.2)
mssa_indicator_obj<-compute_mssa_BIP_predictors_func(x_mat_white_noise,lambda_HP,L,date_to_fit,p,q,ht_mssa_vec,h_vec,f_excess,lag_vec,select_vec_multi)
# 3.3 Check performances:
# Forward-shifted HP-BIP
target_shifted_mat=mssa_indicator_obj$target_shifted_mat
# M-SSA indicators
predictor_mssa_mat<-mssa_indicator_obj$predictor_mssa_mat
# M-MSE
predictor_mmse_mat<-mssa_indicator_obj$predictor_mmse_mat
# For the direct predictor we can specify the macro-indicators in the expanding-window regressions
#   -Note: too complex designs lead to overfitting and thus worse out-of-sample performances
select_direct_indicator<-c("ifo_c","ESI")
perf_obj<-compute_perf_func(x_mat_white_noise,target_shifted_mat,predictor_mssa_mat,predictor_mmse_mat,date_to_fit,select_direct_indicator,h_vec)
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
# We need to check whether we can forecast the white noise data WN (not HP-WN)  based on the predictors
# Full sample
p_value_HAC_BIP_full
# Out-of-sample:
p_value_HAC_BIP_oos
# Check number of occurrences below 1%
length(which(p_value_HAC_BIP_full<0.01))
length(which(p_value_HAC_BIP_oos<0.01))
# Question: does the more flexible design allow to predict BIP (not HP-BIP) more reliably?
# 3.1 Run more adaptive M-SSA design
lambda_HP<-16
# Notes:
# -For adaptive designs, a pronounced left-shift might lead to phase-reversal which is undesirable
# -Therefore we use forecast horizons up to 4 quarters (instead of 6) and no forecast excess
#   -Phase-reversal would be fine (optimal) if the data were in agreement with the implicit assumptions
#     underlying the HP filter (which is not the case, see tutorial 2.0)
f_excess_adaptive<-rep(0,length(select_vec_multi))
h_vec_adaptive<-0:4
# Run the M-SSA predictor function
mssa_indicator_obj<-compute_mssa_BIP_predictors_func(x_mat,lambda_HP,L,date_to_fit,p,q,ht_mssa_vec,h_vec_adaptive,f_excess_adaptive,lag_vec,select_vec_multi)
# Forward-shifted HP-BIP
target_shifted_mat=mssa_indicator_obj$target_shifted_mat
# M-SSA indicators
predictor_mssa_mat<-mssa_indicator_obj$predictor_mssa_mat
# M-MSE
predictor_mmse_mat<-mssa_indicator_obj$predictor_mmse_mat
# 3.2 Compute performances
# For the direct predictor we can specify the macro-indicators in the expanding-window regressions
#   -Note: too complex designs lead to overfitting and thus worse out-of-sample performances
select_direct_indicator<-c("ifo_c","ESI")
perf_obj<-compute_perf_func(x_mat,target_shifted_mat,predictor_mssa_mat,predictor_mmse_mat,date_to_fit,select_direct_indicator,h_vec_adaptive)
p_value_HAC_HP_BIP_full=perf_obj$p_value_HAC_HP_BIP_full
t_HAC_HP_BIP_full=perf_obj$t_HAC_HP_BIP_full
cor_mat_HP_BIP_full=perf_obj$cor_mat_HP_BIP_full
p_value_HAC_HP_BIP_oos=perf_obj$p_value_HAC_HP_BIP_oos
t_HAC_HP_BIP_oos=perf_obj$t_HAC_HP_BIP_oos
cor_mat_HP_BIP_oos=perf_obj$cor_mat_HP_BIP_oos
p_value_HAC_BIP_full=perf_obj$p_value_HAC_BIP_full
t_HAC_BIP_full=perf_obj$t_HAC_BIP_full
cor_mat_BIP_full=perf_obj$cor_mat_BIP_full
p_value_HAC_BIP_oos=perf_obj$p_value_HAC_BIP_oos
t_HAC_BIP_oos=perf_obj$t_HAC_BIP_oos
cor_mat_BIP_oos=perf_obj$cor_mat_BIP_oos
rRMSE_MSSA_HP_BIP_direct=perf_obj$rRMSE_MSSA_HP_BIP_direct
rRMSE_MSSA_HP_BIP_mean=perf_obj$rRMSE_MSSA_HP_BIP_mean
rRMSE_MSSA_BIP_direct=perf_obj$rRMSE_MSSA_BIP_direct
rRMSE_MSSA_BIP_mean=perf_obj$rRMSE_MSSA_BIP_mean
target_BIP_mat=perf_obj$target_BIP_mat
# We now examine performances when targeting specifically BIP (we already know that HP-BIP can be predicted)
# 1. Full sample
cor_mat_BIP_full
# 2. Out-of-sample (specified by date_to_fit above)
cor_mat_BIP_oos
# These numbers are larger than in exercise 1, suggesting that the more adaptive design here is better able to
#   track forward-shifted BIP
# Are the results significant?
p_value_HAC_BIP_full
p_value_HAC_BIP_oos
